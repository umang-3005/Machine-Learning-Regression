
Project Overview:
This project addresses a two-part tabular regression challenge with distinct objectives and deployment constraints:

Part 1: Build a high-performance ML model to predict the continuous target target01 from 273 features

Part 2: Reverse-engineer a rule-based system for target02 that must run on edge devices without ML inference capabilities

Dataset Specifications:

Training samples: 10,000

Features: 273 (feat_1 through feat_273)

Two regression targets: target01 and target02

Repository Structure
text
‚îú‚îÄ‚îÄ Part1.py                      # Main pipeline for target01 prediction
‚îú‚îÄ‚îÄ Part2.py                      # Rule discovery and validation for target02
‚îú‚îÄ‚îÄ parameter_experiment.py       # Hyperparameter tuning experiments
‚îú‚îÄ‚îÄ framework_37.py              # Deployment-ready rule engine for target02
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ EVAL_target01_37.csv        # Final predictions for target01
‚îú‚îÄ‚îÄ Report_ML_W26.pdf           # Complete technical report
‚îú‚îÄ‚îÄ pra_mal_w25.pdf             # Project assignment specification
‚îî‚îÄ‚îÄ problem_37/                 # Dataset directory (not included)
    ‚îú‚îÄ‚îÄ dataset_37.csv          # Training features
    ‚îú‚îÄ‚îÄ target_37.csv           # Training targets
    ‚îî‚îÄ‚îÄ EVAL_37.csv             # Evaluation features
    
Part 1: Predicting target01
Approach
A robust regression pipeline combining feature selection with gradient boosting:
Data Preprocessing: Median imputation for missing values
Feature Selection: ExtraTreesRegressor-based selection (273 ‚Üí 136 features)
Model: HistGradientBoostingRegressor with optimized hyperparameters
Validation: 5-fold cross-validation with out-of-fold predictions

Pipeline Architecture
python
Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('selector', SelectFromModel(
        estimator=ExtraTreesRegressor(n_estimators=400),
        threshold='median'
    )),
    ('reg', HistGradientBoostingRegressor(
        learning_rate=0.05,
        max_leaf_nodes=31,
        min_samples_leaf=10,
        max_iter=900,
        random_state=42
    ))
])
Performance Metrics
Metric	With Feature Selection	Without Selection	Improvement
MAE	0.070452 ¬± 0.004699	0.074450 ¬± 0.005234	-5.37%
RMSE	0.088561 ¬± 0.005073	0.093234 ¬± 0.005229	-5.01%
R¬≤	0.859576 ¬± 0.017345	0.844368 ¬± 0.019170	+1.80%
Running Part 1
bash
python Part1.py
Outputs:

EVAL_target01_37.csv - Predictions for evaluation set

target01_histogram.png - Target distribution visualization

domain_shift_roc.png - Domain shift analysis

Part 2: Rule-Based Prediction for target02
Problem Statement
Predict target02 using only simple conditional rules and arithmetic operations (no ML at runtime) for edge device deployment.

Discovered Rule System
Key Feature: feat_132 (gating feature)
Predictor Features: feat_108, feat_116, feat_255
Thresholds: 0.2, 0.5, 0.7

Rule Formulas
python
if feat_132 ‚â§ 0.2:
    target02 = 1.35√ófeat_108 + 1.75√ófeat_116 - 0.75√ófeat_255

elif feat_132 ‚â§ 0.5:
    target02 = 0.35√ófeat_108 - 0.45√ófeat_116 + 0.55√ófeat_255

elif feat_132 ‚â§ 0.7:
    target02 = 0.15√ófeat_108 + 0.85√ófeat_116 - 1.95√ófeat_255

else:  # feat_132 > 0.7
    target02 = 1.85√ófeat_108 - 1.75√ófeat_116 - 0.75√ófeat_255
Validation Results
Region	Condition	Samples	R¬≤	Max Error
1	‚â§ 0.2	1,999	1.0	~10‚Åª¬π‚Åµ
2	(0.2, 0.5]	2,951	1.0	~10‚Åª¬π‚Åµ
3	(0.5, 0.7]	2,011	1.0	~10‚Åª¬π‚Åµ
4	> 0.7	3,039	1.0	~10‚Åª¬π‚Åµ
Note: Errors at 10‚Åª¬π‚Åµ scale indicate floating-point precision limits, confirming perfect reconstruction.

Running Part 2
bash
# Rule discovery and validation
python Part2.py

# Deploy rules (edge device compatible)
python framework_37.py --eval_file_path problem_37/EVAL_37.csv
Outputs:

rules_target02_37.json - Extracted rule parameters

part2_decision_tree.png - Tree visualization

part2_coefficient_contributions.png - Region-wise coefficients

part2_feature_distribution.png - Split feature analysis

part2_actual_vs_predicted.png - Reconstruction validation

Installation:
Requirements
bash
pip install -r requirements.txt
Dependencies:

numpy >= 1.21.0

pandas >= 1.3.0

scikit-learn >= 1.0.0

matplotlib >= 3.4.0

Python Version
Tested with Python 3.8+

Methodology Highlights
Part 1: Machine Learning Pipeline
Feature Selection Rationale: Reduces overfitting by eliminating 50% of low-importance features using median-threshold ExtraTreesRegressor

Hyperparameter Tuning: GridSearchCV over 243 combinations (5-fold CV)

Validation Strategy: Out-of-fold predictions to prevent leakage

Domain Shift Analysis: ROC-based classifier to detect train/eval distribution differences

Part 2: Rule Discovery Process
Tree Exploration: DecisionTreeRegressor to identify splitting feature and thresholds

Threshold Simplification: Rounding to 0.1 precision for deployable boundaries

Per-Region Regression: LinearRegression fitted independently in each region

Validation: Numerical parity check (reconstruction error ~10‚Åª¬π‚Åµ)

Visualizations
The project generates comprehensive visualizations for analysis:

Part 1: Target distribution, domain shift ROC curve

Part 2: Decision tree structure, coefficient contributions, feature distributions, actual vs predicted comparisons

üî¨ Technical Details
Hyperparameter Search Space
python
param_grid = {
    'reg__learning_rate': [0.03, 0.05, 0.08],
    'reg__max_leaf_nodes': [31, 63, 127],
    'reg__min_samples_leaf': [10, 20, 50],
    'reg__max_iter': [300, 600, 900],
    'reg__l2_regularization': [0.0, 0.1, 1.0]
}
# Total: 243 combinations
Reproducibility
All experiments use fixed random seed: RANDOM_SEED = 42

‚ö†Ô∏è AI Tool Disclosure
Portions of code were edited with assistance from OpenAI ChatGPT (GPT-5.2 Thinking) for:

Debugging assistance

Code refactoring suggestions

Framework interface compliance verification

All logic was independently reviewed and tested by the author.

üìÑ Documentation
Complete technical documentation available in Report_ML_W26.pdf covering:

Detailed methodology for both parts

Mathematical formulations

Validation strategies

Performance analysis

Scientific references

üèÜ Key Achievements
‚úÖ Part 1: Feature selection improved MAE by 5.37% and RMSE by 5.01%
‚úÖ Part 2: Achieved perfect reconstruction (R¬≤ = 1.0) with only 4 simple rules
‚úÖ Edge Deployment: Zero ML dependencies at runtime
‚úÖ Reproducibility: Complete pipeline with fixed random seeds
